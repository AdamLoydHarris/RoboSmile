{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamLoydHarris/RoboSmile/blob/main/Notebooks/RoboSmile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a Jupyter notebook that guides you through the process of generating simulated patient data, fine-tuning a model using Google Generative AI (GEMINI), and evaluating the model's performance in assessing mental health states. The notebook is well-documented with explanatory comments to help you understand each step.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "We start by installing the google-generativeai package, which provides access to Google's Generative AI models.\n",
        "We import necessary libraries:\n",
        "google.generativeai for interacting with the GEMINI API.\n",
        "pandas and numpy for data manipulation.\n",
        "tqdm for progress bars during data generation.\n",
        "We retrieve and configure the API key securely using a userdata module."
      ],
      "metadata": {
        "id": "pg8-WwHuFM39"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8o12wr4I7cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2q43Nl1GFKI1"
      },
      "outputs": [],
      "source": [
        "# Install the google-generativeai package\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install -q --upgrade google-generativeai\n",
        "\n",
        "# Import necessary libraries\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "#import userdata  # Assuming you have a module to handle user data securely\n",
        "\n",
        "# Configure the API Key\n",
        "GOOGLE_API_KEY = 'AIzaSyCNyCdzpcNjEU2vFlhWpQIW0DZfFH_uqwE'\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Simulated Patient Data\n",
        "We'll generate a dataset of simulated patients with various mental health conditions and communication abilities.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "We define a list of mental health conditions and communication levels.\n",
        "The generate_patient_response function creates a prompt for the GEMINI model to generate a patient's response based on the condition and communication level.\n",
        "We loop through each condition and communication level, generating 100 samples for each combination.\n",
        "The data is stored in a pandas DataFrame for easy manipulation."
      ],
      "metadata": {
        "id": "14CnVPICFaxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define mental health conditions and communication levels\n",
        "mental_health_conditions = [\n",
        "    'Depression',\n",
        "    'Anxiety',\n",
        "    'Bipolar Disorder',\n",
        "    'Schizophrenia',\n",
        "    'PTSD',\n",
        "    'OCD'\n",
        "]\n",
        "\n",
        "communication_levels = ['Low', 'Medium', 'High']\n",
        "\n",
        "\n",
        "#def generate_patient_response(condition, communication_level):\n",
        "#    prompt = f\"Patient with {condition} and communication level {communication_level}: How have you been feeling lately?\"\n",
        "#    response = model.generate_content(prompt)\n",
        "#    return response.text\n",
        "\n",
        "# Generate the dataset\n",
        "#data = []\n",
        "\n",
        "#for condition in tqdm(mental_health_conditions):\n",
        "#    for comm_level in communication_levels:\n",
        "#        for _ in range(100):  # Generate 100 samples per condition and communication level\n",
        "#            response = generate_patient_response(condition, comm_level)\n",
        "#            data.append({\n",
        "#                'Condition': condition,\n",
        "#                'CommunicationLevel': comm_level,\n",
        "#                'Response': response\n",
        "#            })\n",
        "\n",
        "# Create a DataFrame\n",
        "#df = pd.DataFrame(data)\n",
        "\n",
        "def generate_patient_response(condition, communication_level):\n",
        "    prompt = f\"Patient with {condition} and communication level {communication_level}: How have you been feeling lately?\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        # Check if the response contains candidates\n",
        "        if not response or not hasattr(response, 'candidates') or len(response.candidates) == 0:\n",
        "            return None\n",
        "\n",
        "        # Get the first candidate text\n",
        "        candidate = response.candidates[0]\n",
        "        if hasattr(candidate, 'text'):\n",
        "            return candidate.text\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except AttributeError as e:\n",
        "        print(f\"AttributeError: {e}. Condition: {condition}, Communication Level: {communication_level}\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}. Condition: {condition}, Communication Level: {communication_level}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Generate the dataset\n",
        "data = []\n",
        "\n",
        "for condition in tqdm(mental_health_conditions):\n",
        "    for comm_level in communication_levels:\n",
        "        for _ in range(100):  # Generate 100 samples per condition and communication level\n",
        "            response = generate_patient_response(condition, comm_level)\n",
        "\n",
        "            if response is not None:\n",
        "              data.append({\n",
        "                  'Condition': condition,\n",
        "                  'CommunicationLevel': comm_level,\n",
        "                  'Response': response\n",
        "              })\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "G7MLDD2dc2hy",
        "outputId": "f6dd947a-8fd3-48ec-b877-21632a68aabb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [59:06<00:00, 591.01s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('mental_health_data2.csv', index=False)\n",
        "\n",
        "print(\"Data has been saved to 'mental_health_data2.csv'.\")\n"
      ],
      "metadata": {
        "id": "XjRLZbVs2Ydb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce49159-7be5-4b4a-e8b6-a22af1c52fb7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to 'mental_health_data2.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DataFrame from the CSV file\n",
        "df_loaded = pd.read_csv('mental_health_data2.csv')\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(df_loaded.head())"
      ],
      "metadata": {
        "id": "m4iYZmaG7-ps",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6eecd0da-6985-4365-9958-5d138fbc3793"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EmptyDataError",
          "evalue": "No columns to parse from file",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e79068c02a1a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the DataFrame from the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mental_health_data2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display the first few rows to verify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the Data\n",
        "Before fine-tuning the model, we'll preprocess the data.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "We encode the communication levels and conditions numerically to prepare for model training.\n",
        "We define a clean_response function to preprocess the text if necessary."
      ],
      "metadata": {
        "id": "VdS3TMRpFbqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first few rows\n",
        "df.head()\n",
        "\n",
        "# Encode communication levels\n",
        "comm_level_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
        "df['CommunicationLevelEncoded'] = df['CommunicationLevel'].map(comm_level_mapping)\n",
        "\n",
        "# Encode conditions\n",
        "condition_mapping = {condition: idx for idx, condition in enumerate(mental_health_conditions)}\n",
        "df['ConditionEncoded'] = df['Condition'].map(condition_mapping)\n",
        "\n",
        "# Clean the responses (optional)\n",
        "# For example, remove any prompts or irrelevant text if present\n",
        "##def clean_response(text):\n",
        "    # Implement any cleaning steps if necessary\n",
        "#    return text.strip()\n",
        "\n",
        "df['CleanedResponse'] = df['Response']#.apply(clean_response)\n"
      ],
      "metadata": {
        "id": "20HkfwOKFb21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the Data into Training and Testing Sets\n",
        "\n",
        "Explanation:\n",
        "\n",
        "We use train_test_split from scikit-learn to split the data into training and testing sets.\n",
        "We prepare separate labels for condition and communication level.\n"
      ],
      "metadata": {
        "id": "XH4HUpYsGWhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Features and labels\n",
        "X = df['CleanedResponse']\n",
        "y_condition = df['ConditionEncoded']\n",
        "y_comm_level = df['CommunicationLevelEncoded']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train_condition, y_test_condition = train_test_split(\n",
        "    X, y_condition, test_size=0.2, random_state=42)\n",
        "\n",
        "_, _, y_train_comm_level, y_test_comm_level = train_test_split(\n",
        "    X, y_comm_level, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "UUD0JyP8GXe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for None values in X_train and X_test: Ensure that neither dataset contains None values. You can use the following code to check:\n",
        "\n",
        "#print(any(x is None for x in X_train))  # Check for None in X_train\n",
        "#print(any(x is None for x in X_test))   # Check for None in X_test\n",
        "\n",
        "#Replace None values: If None values exist, replace them with an empty string or a default value:\n",
        "#X_train = [x if x is not None else '' for x in X_train]\n",
        "#X_test = [x if x is not None else '' for x in X_test]\n",
        "\n",
        "#Validate Data: Ensure that all elements in X_train and X_test are strings. You can convert them to strings explicitly if needed:\n",
        "#X_train = [str(x) for x in X_train]\n",
        "#X_test = [str(x) for x in X_test]\n"
      ],
      "metadata": {
        "id": "fRhj4V1v-q1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensure that X_train contains meaningful text. Check for empty strings or strings with only stop\n",
        "#print(X_train[:5])  # Print a sample of X_train\n",
        "#print([x for x in X_train if len(x.strip()) == 0])  # Check for empty strings\n",
        "\n",
        "#If X_train contains empty or stop-word-only strings, remove or replace them:\n",
        "#X_train = [x for x in X_train if len(x.strip()) > 0]  # Remove empty strings\n",
        "#X_train = [x if len(x.strip()) > 0 else \"placeholder\" for x in X_train]  # Replace empty with placeholder\n",
        "\n",
        "#By default, TfidfVectorizer removes stop words. You can disable this behavior or provide a custom list of stop words:\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#vectorizer = TfidfVectorizer(max_features=5000, stop_words=None)  # Disable stop word removal"
      ],
      "metadata": {
        "id": "0nwQKMRo_w6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the Text Data\n",
        "We'll convert the text data into numerical vectors using TF-IDF.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "We use TF-IDF to vectorize the text responses.\n",
        "The vocabulary is built on the training data and then applied to the test data."
      ],
      "metadata": {
        "id": "PUb3dCIYGXq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit and transform the training data, transform the test data\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "XKguA_HaGszl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Classifier Model\n",
        "We'll train a machine learning model to predict the mental health condition based on the patient's response.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "We use Logistic Regression for multiclass classification.\n",
        "We train the model on the vectorized training data and evaluate it on the test set.\n",
        "The classification report shows precision, recall, and F1-score for each condition."
      ],
      "metadata": {
        "id": "7KIqcs4yGtEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize the model\n",
        "model_condition = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model_condition.fit(X_train_vect, y_train_condition)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_condition = model_condition.predict(X_test_vect)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report for Mental Health Condition Prediction:\")\n",
        "print(classification_report(y_test_condition, y_pred_condition, target_names=mental_health_conditions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "MF5V20NwGtPp",
        "outputId": "5e2280b6-bd3d-4f85-f9d0-1280a32de50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_vect' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a6e2897f91ab>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_condition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Predict on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_vect' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Model for Communication Level Prediction\n",
        "Similarly, we can train a model to predict the communication level.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "We train another Logistic Regression model to predict the communication level.\n",
        "Evaluation metrics are displayed similarly."
      ],
      "metadata": {
        "id": "J1Fuf4LVG5Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_comm_level = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model_comm_level.fit(X_train_vect, y_train_comm_level)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_comm_level = model_comm_level.predict(X_test_vect)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report for Communication Level Prediction:\")\n",
        "print(classification_report(y_test_comm_level, y_pred_comm_level, target_names=communication_levels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "bqJ8WY_DG5oB",
        "outputId": "a790ba6a-a5c1-4029-a745-4382157077cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_vect' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d13188ba3e9e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_comm_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_comm_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Predict on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_vect' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning with GEMINI (Optional)\n",
        "If GEMINI supports fine-tuning, we can proceed to fine-tune the model using our dataset.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "As of my knowledge cutoff, fine-tuning may not be directly available through the GEMINI API.\n",
        "If fine-tuning is supported, you'd prepare your data accordingly and use the appropriate function.\n",
        "In this notebook, we'll proceed with our custom-trained machine learning models."
      ],
      "metadata": {
        "id": "Nyx5bHhfHFFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GEMINI supports fine-tuning (this is hypothetical)\n",
        "# GEMINI may not support fine-tuning via the API directly\n",
        "# If supported, the code might look like this:\n",
        "\n",
        "# Prepare the data in the required format\n",
        "training_data = df[['CleanedResponse', 'Condition']].values.tolist()\n",
        "\n",
        "# Fine-tune the model (hypothetical function)\n",
        "# genai.fine_tune_model(training_data=training_data, model_name='your-custom-model')\n",
        "\n",
        "# Since fine-tuning might not be available, we proceed without it\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xnx2qouFHJFi",
        "outputId": "9697aa1a-45f4-45e5-8fb0-22fed0c9f0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c7fb51c4bea0>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Prepare the data in the required format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CleanedResponse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Condition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Fine-tune the model (hypothetical function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Reward Function\n",
        "We'll define a reward function to evaluate whether our tool accurately assesses the agent's mental state despite communication difficulties.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "The reward_function assigns rewards based on prediction correctness and communication level.\n",
        "We calculate the rewards for each sample in the test set and compute the average reward."
      ],
      "metadata": {
        "id": "XTPVfF9cHSL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_function(true_condition, predicted_condition, true_comm_level):\n",
        "    # Assign higher rewards for correct predictions on low communication levels\n",
        "    if true_condition == predicted_condition:\n",
        "        if true_comm_level == 0:  # Low communication ability\n",
        "            return 2  # Higher reward\n",
        "        else:\n",
        "            return 1  # Standard reward\n",
        "    else:\n",
        "        return -1  # Penalty for incorrect prediction\n",
        "\n",
        "# Calculate rewards for the test set\n",
        "rewards = []\n",
        "for i in range(len(y_test_condition)):\n",
        "    reward = reward_function(\n",
        "        y_test_condition.iloc[i],\n",
        "        y_pred_condition[i],\n",
        "        y_test_comm_level.iloc[i]\n",
        "    )\n",
        "    rewards.append(reward)\n",
        "\n",
        "average_reward = np.mean(rewards)\n",
        "print(f\"Average Reward: {average_reward}\")\n"
      ],
      "metadata": {
        "id": "60q-H10oHQ05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide Feedback for the General Practitioner (GP)\n",
        "Finally, we'll simulate how the tool provides feedback to the GP for establishing follow-up care.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "The generate_gp_feedback function takes a patient's response and provides feedback for the GP.\n",
        "It predicts the condition and communication level, then formats a recommendation.\n",
        "We demonstrate this with a sample response from the test set.\n",
        "\n"
      ],
      "metadata": {
        "id": "15PCmZnyHan2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_gp_feedback(patient_response):\n",
        "    # Use the model to predict the condition and communication level\n",
        "    response_vect = vectorizer.transform([patient_response])\n",
        "    predicted_condition = model_condition.predict(response_vect)[0]\n",
        "    predicted_comm_level = model_comm_level.predict(response_vect)[0]\n",
        "\n",
        "    condition_name = [k for k, v in condition_mapping.items() if v == predicted_condition][0]\n",
        "    comm_level_name = [k for k, v in comm_level_mapping.items() if v == predicted_comm_level][0]\n",
        "\n",
        "    feedback = f\"\"\"\n",
        "    Based on the patient's response, the predicted mental health condition is {condition_name},\n",
        "    and their communication ability is {comm_level_name}.\n",
        "\n",
        "    Recommended follow-up: Refer the patient to a specialist in {condition_name}.\n",
        "    \"\"\"\n",
        "    return feedback\n",
        "\n",
        "# Example usage\n",
        "sample_response = X_test.iloc[0]\n",
        "feedback = generate_gp_feedback(sample_response)\n",
        "print(\"GP Feedback:\")\n",
        "print(feedback)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "3O4CTuM-HeT0",
        "outputId": "5e8c9477-52af-49fa-c2d0-f9b2e108172c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-09b15b44e140>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msample_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mfeedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_gp_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GP Feedback:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "In this notebook, we've:\n",
        "\n",
        "Generated simulated patient responses using GEMINI.\n",
        "Preprocessed and vectorized the data.\n",
        "Trained machine learning models to predict mental health conditions and communication abilities.\n",
        "Defined a reward function to evaluate the model's performance.\n",
        "Created a function to provide actionable feedback for general practitioners.\n",
        "Note: Ensure that you comply with all relevant data protection regulations when handling real patient data. The simulated data in this notebook is generated for educational purposes."
      ],
      "metadata": {
        "id": "LKkZWsw2HlVD"
      }
    }
  ]
}